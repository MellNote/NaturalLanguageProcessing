{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " NaturalLanguageProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcfUoDQ71Ca9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ea3f80b-96f7-456b-b878-336ae50fdab9"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import joblib\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDVQ5FzFujc1",
        "colab_type": "text"
      },
      "source": [
        "単語ベクトルの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjCuOD3dN6cR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_word2id():\n",
        "    word2id = {} #ディクショナリ word2id = {'単語':単語id}\n",
        "\n",
        "    with open('drive/My Drive/train.txt', 'r', encoding='utf_8') as f:\n",
        "        morphemes = [s.strip()[1:] for s in f.readlines()]\n",
        "\n",
        "    for line in morphemes:\n",
        "        for word in line.split():\n",
        "            if word not in word2id:\n",
        "                word2id[word] = len(word2id)\n",
        "\n",
        "    \n",
        "\n",
        "    with open('drive/My Drive/dic.txt', 'w', encoding='utf_8') as f2:\n",
        "        for word in word2id:\n",
        "            f2.write('{},{}\\n'.format(word, word2id[word]))\n",
        "    return word2id\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys3Mu40iFVJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8d31baf-eadc-42b3-ff16-d1eae1b2043c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k_10_lGpjgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_feature(word2id):\n",
        "    for text_name in ['train', 'test']:\n",
        "\n",
        "        with open('drive/My Drive/' + text_name + '.txt', 'r', encoding='utf_8') as f:\n",
        "            morphemes = [s.strip()[1:] for s in f.readlines()]\n",
        "            word_array = [0]*(len(word2id))\n",
        "            word_array2 = [] \n",
        "\n",
        "            for line in morphemes:\n",
        "                for word in line.split():\n",
        "                    if(word in word2id):\n",
        "                        word_array[word2id[word]]=1\n",
        "                \n",
        "                word_array2.append(word_array)\n",
        "                word_array = [0]*(len(word2id))\n",
        "\n",
        "        with open('drive/My Drive/' + text_name + '_feature.txt', 'w') as f2:\n",
        "     \n",
        "            for line in range(len(word_array2)):\n",
        "                maped_list=map(str,word_array2[line])\n",
        "                mojiretu=','.join(maped_list)\n",
        "\n",
        "                f2.write(mojiretu)\n",
        "                f2.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwb1u7yv7Ojv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 辞書(単語と単語idのリスト)の作成\n",
        "word2id = make_word2id()\n",
        "\n",
        "# テキストデータをベクトルに変換\n",
        "make_feature(word2id)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzD4T_CmNEvk",
        "colab_type": "text"
      },
      "source": [
        "モデルの学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVVqemFJ69ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(X_text,y_text):\n",
        "    # 特徴量の読み込み\n",
        "    with open(X_text,encoding=\"utf_8\") as f:\n",
        "        reader = csv.reader(f, delimiter=',')\n",
        "        X_data = [row for row in reader]\n",
        "\n",
        "    # カテゴリーIDの読み込み\n",
        "    with open(y_text,encoding=\"utf_8\") as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        y_data = [row[0] for row in reader]\n",
        "\n",
        "    # str型をfloat型に変換\n",
        "    for i in range(len(X_data)):\n",
        "        X_data[i] = [float(n) for n in X_data[i]]\n",
        "\n",
        "    y_data = [float(n) for n in y_data]\n",
        "\n",
        "    return X_data,y_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmo0tGgrwhXL",
        "colab_type": "text"
      },
      "source": [
        "# 新しいセクション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWeqOdeTuPEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 学習データの読み込み\n",
        "X_train,y_train = load_data('drive/My Drive/train_feature.txt','drive/My Drive/train.txt')\n",
        "\n",
        "# 評価データの読み込み\n",
        "X_test,y_test = load_data('drive/My Drive/test_feature.txt','drive/My Drive/test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ6lj8RkNT1I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5f22374-4d62-4fe4-dd63-16f400098db9"
      },
      "source": [
        "# ロジスティック回帰モデルを学習\n",
        "lr = LogisticRegression(C=0.1) #正則化パラメータを0.1\n",
        "#lr = LogisticRegression()\n",
        "lr.fit(X_train,y_train)\n",
        "joblib.dump(lr, 'drive/My Drive/model.joblib')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/My Drive/model.joblib']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs0W703GbI7v",
        "colab_type": "text"
      },
      "source": [
        "カテゴリ推定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROc7IARMbQC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 学習モデルの読み込み\n",
        "lf = joblib.load('drive/My Drive/model.joblib')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYwIR3CScxL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8f0719a-aec2-4117-ec79-d8edc6e5c22b"
      },
      "source": [
        "# 正解率表示\n",
        "print(f'Accuracy: {accuracy_score(y_train, lf.predict(X_train))}')\n",
        "\n",
        "##lf.predict(X_test)で推定したカテゴリにアクセス可能\n",
        "res = lf.predict(X_test)\n",
        "print(res)\n",
        "proba = lf.predict_proba(X_train)\n",
        "print(proba)\n",
        "\n",
        "#特徴量の重みの確認\n",
        "# category = ['Business', 'Entertainment', 'Science and Technology', 'Health']\n",
        "# id2word = {v: k for k, v in word2id.items()}\n",
        "# for list in range(lf.coef_.shape[0]):\n",
        "#     for id in id2word:\n",
        "#         print(f'{category[list]}:{id2word[id]}:{lf.coef_[list][id]}')\n",
        "\n",
        "\n",
        "#### 出力部分）####\n",
        "\n",
        "category = ['Business', 'Entertainment', 'Science and Technology', 'Health']\n",
        "id2word = {v: k for k, v in word2id.items()}\n",
        "for list in range(lf.coef_.shape[0]):\n",
        "  largest_index = np.argsort(lf.coef_[list][:])[-11:-1]\n",
        "  smallest_index = np.argsort(lf.coef_[list][:])[0:9]\n",
        "  \n",
        "  print(category[list] + \" largest:\")\n",
        "  print([id2word[x] for x in largest_index])\n",
        "  print([lf.coef_[list][x] for x in largest_index])\n",
        "\n",
        "  print(category[list] + \" smallest:\")\n",
        "  print([id2word[x] for x in smallest_index])\n",
        "  print([lf.coef_[list][x] for x in smallest_index])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.695\n",
            "[1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
            " 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
            " 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0.\n",
            " 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1.\n",
            " 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
            " 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0.\n",
            " 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0.\n",
            " 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.\n",
            " 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.\n",
            " 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1.\n",
            " 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
            " 1. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0.\n",
            " 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
            " 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0.\n",
            " 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0.\n",
            " 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0.\n",
            " 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n",
            " 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
            " 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n",
            " 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
            " 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0.\n",
            " 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n",
            " 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0.\n",
            " 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
            " 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1.\n",
            " 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.\n",
            " 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1.\n",
            " 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1.\n",
            " 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
            "[[0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " ...\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]]\n",
            "Business largest:\n",
            "['bln', 'China', 'at', 'of', 'for', 'in', 'on', 'as', 'UPDATE', 'US']\n",
            "[6.744639837646276e-12, 7.511076182833352e-12, 7.970937989945598e-12, 9.043948873207505e-12, 1.3795854213367382e-11, 1.816454138093372e-11, 2.306973399013101e-11, 2.5062468487617412e-11, 2.567561756376707e-11, 2.805157023384701e-11]\n",
            "Business smallest:\n",
            "['...', '-', 'The', 'To', 'and', 'Kardashian', 'And', 'Of', 'For']\n",
            "[-1.4715577827591874e-11, -1.088339610165649e-11, -7.051214375721107e-12, -5.1351235127534146e-12, -4.062112629491507e-12, -3.755538091416676e-12, -3.678894456897969e-12, -3.6022508223792608e-12, -3.065745380748307e-12]\n",
            "Entertainment largest:\n",
            "['For', 'And', 'In', 'A', 'Of', 'Kardashian', 'and', 'To', 'The', '-']\n",
            "[9.503810680319752e-12, 9.810385218394583e-12, 1.0423534294544244e-11, 1.0806752467137784e-11, 1.1113327005212613e-11, 1.1266614274250028e-11, 1.157318881232486e-11, 1.4485646924035751e-11, 2.0847068589088488e-11, 3.816852999031642e-11]\n",
            "Entertainment smallest:\n",
            "['UPDATE', 'US', 'on', 'China', 'bln', 'ECB', '(1)', 'says', 'Ukraine']\n",
            "[-1.2033050619437106e-11, -9.963672487431998e-12, -4.521974436603753e-12, -2.912458111710892e-12, -2.4525963045986457e-12, -2.1460217665238152e-12, -2.0693781320051074e-12, -1.916090862967692e-12, -1.7628035939302767e-12]\n",
            "Science and Technology largest:\n",
            "['Xbox', 'Everything', 'Watch', 'Comcast', \"Google's\", 'Samsung', 'FCC', 'Facebook', 'Apple', 'GM']\n",
            "[9.197236142244922e-13, 9.197236142244922e-13, 9.963672487431998e-13, 1.0730108832619076e-12, 1.1496545177806152e-12, 1.2262981522993229e-12, 1.3795854213367381e-12, 1.6095163248928612e-12, 2.0693781320051074e-12, 2.605883573636061e-12]\n",
            "Science and Technology smallest:\n",
            "['...', '-', 'to', 'as', 'in', 'for', 'US', 'of', 'on']\n",
            "[-1.808789774641501e-11, -1.3949141482404797e-11, -1.3489279675292552e-11, -1.325934877173643e-11, -1.1266614274250028e-11, -8.584087066095261e-12, -8.430799797057844e-12, -6.5913525686088605e-12, -6.3614216650527375e-12]\n",
            "Health largest:\n",
            "['deaths', 'Health', 'Guinea', 'FDA', 'drugs', 'Study', 'drug', 'study', 'can', 'MERS']\n",
            "[6.897927106683691e-13, 8.430799797057845e-13, 9.197236142244922e-13, 9.197236142244922e-13, 9.963672487431998e-13, 1.0730108832619076e-12, 1.0730108832619076e-12, 1.1496545177806152e-12, 1.1496545177806152e-12, 1.3795854213367381e-12]\n",
            "Health smallest:\n",
            "['...', 'to', 'in', '-', 'on', 'as', 'US', 'of', 'UPDATE']\n",
            "[-1.9620770436789166e-11, -1.9007621360639503e-11, -1.4025785116923505e-11, -1.3335992406255136e-11, -1.218633788847452e-11, -1.1113327005212613e-11, -9.657097949357168e-12, -9.350523411282337e-12, -9.273879776763629e-12]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}